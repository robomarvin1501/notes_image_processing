\documentclass[a4paper]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{tabularx}
\usepackage[autostyle=true]{csquotes}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\B}{\ensuremath{\mathbbm{B}}}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\title{Tutorial 9 - Neural Networks}
\author{Gidon Rosalki}
\date{2025-12-17}


\begin{document}
\maketitle

\section{Learning}\label{sec:learning} % (fold)
The traditional goal of learning is to learn a mapping $f \left(x\right)$ from a domain set $ \mathcal{X} $ to a label
set $\mathcal{Y}$. There are two common settings: \begin{itemize}
    \item Classification: Here $\mathcal{Y}$ is discrete, where each $y \in \mathcal{Y}$ represents a class 
    \item Regression: $\mathcal{Y}$ is continuous
\end{itemize}

\begin{table}[H]
     \centering
     \begin{tabular}{|p{0.27\textwidth}|p{0.27\textwidth}|p{0.27\textwidth}|}
         \hline
         $\mathcal{X}$ & $\mathcal{Y}$ & $f \left(x\right)$ \\ \hline
         Images of an animal & \{Cat, dog, deer, $\dots$\} & Recognise the animal in the image \\ \hline
         Sounds of speech & Words in English & Converts speech to text \\ \hline
         Images of faces & $\R ^ {2}$ & Returns location of the mouth in the image \\ \hline
         Corrupted images & Natural images & Restores a corrupted image to its original version \\ \hline 
     \end{tabular}
     \caption{Examples}
\end{table} 

To learn \textit{parametric functions}, we have a training set which is a collection of labelled samples: \[
    S = \left\{\left(x_i \in \mathcal{X}, y_i \in \mathcal{Y}\right)\right\}_{i = 1} ^ {n}
\]
The \textit{hypothesis space} is a set of parametric functions: \[
    \mathcal{H} = \left\{f_{\theta} \left(x\right) | \theta \in \R ^ {S}\right\}
\]
This could be: \begin{itemize}
    \item Linear functions where $f \left(x\right) = \displaystyle\sum_{i = 1}^{S}x_i \cdot w_i$, parametrised by a
        vector $w \in \R ^ {S}$
    \item Linear transformations $f \left(x\right) = A x$, parametrised by a matrix $A \in \R ^ {d \times S}$
    \item Neural networks
\end{itemize}
Our \textbf{goal} is to find parameters $\theta$ such that $f_\theta \in \mathcal{H}$ is the \enquote{best fit}  for the
training set $S$. Not all parameters are learned, there are also hyper-parameters that we set beforehand.

\subsection{Best fit}\label{sub:best_fit} % (fold)
We need to consider what it means to be a \enquote{best fit}. We need to define how good a fit is, so to do this we
define a loss function $L \left(f, S\right)$, which measures the \enquote{error}  of $f$ with respect to $S$. A lower
loss will mean a better fit. This has transformed our \textbf{goal} to finding $f^*$ that minimises the loss: \[
    f^* =  \displaystyle\argmin_{f \in \mathcal{H}} \left\{L \left(f, S\right)\right\} 
\]
For example: \begin{itemize}
    \item \textbf{0 - 1 Loss}: the percentage of examples on which $f$ is wrong: \[
            L_{0 - 1} \left(f, S\right) = \displaystyle\frac{\text{number of times } f \left(x_i\right) \ne y_i}{
            \left|S\right|}
        \]
        This is not particularly used, since it is not great.
    \item \textbf{Mean squared error}: Average distance of $f \left(x_i\right)$ from $y_i$ \[
            L_{MSE} \left(f, S\right) = \displaystyle\frac{1}{\left|S\right|} \displaystyle\sum_{i}^{} \left|\left|f
            \left(x_i\right) - y_i\right|\right| ^ {2}
        \]
    \item \textbf{Cross Entropy Loss}: Assume that $f \left(x\right)$ outputs the probabilities of $x$ belonging to each
        possible class. Use the probability of choosing the correct class on all examples. \[
            \text{Loss} = - \displaystyle\frac{1}{n} \displaystyle\sum_{j = 1}^{n} \log \left(f \left(x_{j,
            \text{correct}}\right)\right)
        \]
        Where $f \left(x_{j, \text{correct}}\right)$ is the probability assigned by the model to the correct class for
        $x_i$
\end{itemize}

So as we can see, we want to minimise the loss function. If $L \left(f, S\right)$ is a differentiable function, then we
can use \textbf{Gradient Descent}. The key principle here is that at each iteration $i$ of GD, we take a step in the
direction of the steepest descent: \[
    \theta_i = \theta_{i - 1} - \eta \triangle f_{\theta_{i - 1}}
\]
The step size is the learning rate, called $\eta$, is a hyper-parameter. You need to pick a value that is not too large
(will miss the minimum), and not too small (will take forever to finish).

If $\left|S\right|$ is large, then every GD iteration is expensive. Instead, we approximate it using a small, randomly
chosen batch $B \subset S$. This is called \textbf{stochastic gradient descent}. This has the loop: \begin{enumerate}
    \item Sample a random batch $B \subset S$ 
    \item Compute the gradient of $L \left(f, B\right)$ with respect to the parameters of $f$ 
    \item Update parameters of $f$ using the gradient
\end{enumerate}
We perform enough steps until we have covered the entire dataset, called an \textbf{epoch}. We perform multiple epochs
during training. Original SGD could be difficult to use for non experts, so instead ADAM is a variant of SGD which is
easier to use in practice.

\subsubsection{Generalisation and overfitting}\label{sec:generalisation_and_overfitting} % (fold)
Let us assume that we have a model that fits the training set. We do not know if it is any good, it could
\textbf{overfit} the data, and be terrible for unseen data as a result. In order to avoid this, we must evaluate the
model on unseen data. To do this, before training we split our available data into training, validation, and test sets.
Then, when training, we choose hyper-parameters using the validation set.
% subsubsection Generalisation and overfitting (end)
% subsection Best fit (end)

\subsection{Summary}\label{sub:summary} % (fold)
We want to fit a parametric function $f_\theta \left(x\right)$ to a training set $S$. We measure the fitness with a loss
function $L \left(f_\theta, S\right)$:\begin{itemize}
    \item For classification we generally use cross entropy loss 
    \item For regression we typically use mean squared error
\end{itemize} 
We find the best $f_\theta$ by minimising the loss. For minimisation we use SGD, or one of its variants. Models must be
evaluated on unseen data in order to avoid overfitting. 
% subsection Summary (end)
% section Learning (end)

\section{Neural Networks}\label{sec:neural_networks} % (fold)
In broad terms, a neural network is a directed acyclic graph of differentiable operations. Each node is called a
\textit{layer}, defined by its type and parameters. All the parameters together are the \textit{weights} of the network.
The \textit{depth} of the network is the longest path from the input to the output. When learning, we fix the structure,
or \textit{architecture}, and only learn the weights. A network is called \textit{feed-forward} if every node has at
most one input, and one output connected. 

Intermediate results are \textit{Tensors}. Tensors are a fancy term fora  multi dimensional array $A \in \R ^ {M_1
\times \dots \times M_n}$. $A$ is an $nD$ tensor with the shape being the tuple $\left(M_1, \dots, M_n\right)$. Images
are 3D tensors of the shape $\left(\text{channels}, \text{height}, \text{width}\right)$. For RGB, then channels = 3, and
for greyscale channels = 1. The convention in PyTorch is NCHW (batch size, channels, height, width), but some other
frameworks use NHWC (Keras). The input and output of each layer are expressed as tensors. The shape and dimension of the
output depends on the type of the layer. 

\subsection{Common layers}\label{sub:common_layers} % (fold)
\subsubsection{Linear / Fully connected}\label{sec:linear_fully_connected} % (fold)
The \textbf{operation} is $f \left(x\right) = Ax + b$. The \textbf{input} is some assumed $x \in \R ^ {N}$, and the
\textbf{output} is $y \in \R ^ {M}$. The \textbf{hyper-parameter} is $M$, the output dimension. It has the
\textbf{learned parameters} of the weights matrix $A \in \R ^ {N \times M}$, and the bias vector $b \in \R ^ {M}$
% subsubsection Linear / Fully connected (end)

\subsubsection{Activation}\label{sec:activation} % (fold)
The \textbf{operation} is to apply a point wise non-linear function $\sigma: \R \to \R$, called the \textit{activation
function}, on every entry (neuron) of the input tensor: \begin{itemize}
    \item ReLU: $\sigma \left(z\right) = \displaystyle\max_{} \left\{z, 0\right\} $
    \item Hyperbolic tangent: $\sigma \left(z\right) = \tanh \left(z\right)$
    \item Sigmoid: $\sigma \left(z\right) = \displaystyle\frac{1}{1 + \exp \left(-z\right)}$
\end{itemize}
This takes the \textbf{input} of any tensor of some shape, and returns an \textbf{output} that is the same shape as the
input. It typically takes no \textbf{hyper-parameters}, but there is an exception of LeakyReLU: $\sigma \left(z\right) =
\displaystyle\max_{} \left\{z, 0.01 z\right\} $. There are also typically no \textbf{learned parameters}, with the
exception of PReLU: $\sigma \left(z\right)= \displaystyle\max_{} \left\{z, \alpha z\right\} $ where $\alpha$ is learned
% subsubsection Activation (end)

\subsubsection{Convolution}\label{sec:convolution} % (fold)
The \textbf{operation} is to convolve the input with a set of kernels. We will run the convolution on each of the layers
independently, then combine them together. Each different kernel will give us a different output channel. It takes the
\textbf{input} of a 3D tensor of the shame $\left(\text{in\_channels}, \text{height}, \text{width}\right)$, and
\textbf{outputs} a 3D tensor of the shape $\left(\text{out\_channels}, \text{height}, \text{width}\right)$. The
\textbf{hyper-parameters} are \begin{enumerate}
    \item A spatial shape of kernel $k \times k$ 
    \item Number of kernels $M$ (number of output channels)
\end{enumerate}
It has the following \textbf{learned parameters}: For $1 \leq j \leq M$ the kernel $w_j \in \R ^ {C \times K \times K}$,
where $C$ is the number of input channels. 
% subsubsection Convolution (end)

\subsubsection{Pooling}\label{sec:pooling} % (fold)
The \textbf{operation} is to sub-sample each input channel. There are two main types: \begin{itemize}
    \item Average pooling: Replace every window with its average value 
    \item Max pooling: REplace eevery window iwth its maximum value
\end{itemize}

The \textbf{input} is a 3D tensor of shape $\left(\text{in\_channels}, \text{height}, \text{width}\right)$, and returns
an \textbf{output} which is a 3D tensor of the shape $\left(\text{in\_channels},
\displaystyle\frac{\text{height}}{\text{window height}}, \displaystyle\frac{\text{width}}{\text{window width}}\right)$.
It takes the \textbf{hyper-parameter} of the size of the pooling window (typically $2 \times 2$), and has no
\textbf{learned parameters}.


% subsubsection Pooling (end)
% subsection Common layers (end)

% section Neural Networks (end)

\section{PyTorch}\label{sec:pytorch} % (fold)
There are many deep learning frameworks, like keras, tesnorflow, PyTorch, and so on. We will be using PyTorch in this
course. Teh API is very similar to numpy. The data, and models are loaded by default to the CPU. NN computations are
very computationally expensive, and slow on the CPU, but well aimed for GPUs, so we will prefer running on the GPU,
where computations are much faster. In PyTorch, we have to explicitly move tensors and the model to the GPU (and back to
CPU). Datasets and Neural networks are written as classes. % TODO new slides with PyTorch, may not need the slides
% section PyTorch (end)

\section{Classification}\label{sec:classification} % (fold)
To classify, we need datasets from which to learn. For example, there is MNIST, a very large dataset of handwritten
digits, each is a $28 \times 28$ greyscale image. There are 60,000 training images, and 10,000 test images. There is
also CIFAR10, which has 10 categories aeroplane, vehicle, bird, cat, deer, dog, frog, horse, ship, and truck. Each
image is a $32 \times 32$ colour image, and there are 50,000 training images, and 10,000 test images. \\ 
There is also IMAGENET, which has 1,000 classes, for $256 \times 256$ colour images, and around 1.2M of them.

Data augmentation is a technique to artificially enlarge the training set. For example, we can take images, and add
flips, rotations, crops, colour jitter, noise, and so on. In short, we apply simple transformations, while keeping the
label the same. This helps reduce overfitting, and improve generalisation.

\subsection{CNNs}\label{sub:cnns} % (fold)
This is a convolutional neural network. CNN is a general term for neural networks that apply convolutions. They
typically use convolution layers in the early stages, and linear (fully connected) layers in the final stages. \\ 
In these networks, the \textit{receptive field} is the region of the input that influences a specific neuron's
activation. So if we convolve with a $3 \times 3$ window each layer, then in layer 2 the receptive field is $3 \times
3$, but in layer 3 it will be $5 \times 5$: 
\begin{figure}[H]
    \center
    \includegraphics[scale=0.2]{tutorial_9_receptive_field.png}
    \caption{Receptive fields}
\end{figure}

We also use something called \textit{dropout}. This is another technique to avoid overfitting. Here, we randomly drop
units (e.g. neurons) during training, which prevents co adaptations. Units are trained according to a loss that is
dependent on all units (i.e. what all the units are doing). Units may change to \enquote{fix} the mistakes of other
units. This builds complex adaptations that do not generalise to unseen data. Model combination improves performance,
and dropout approximates combining many different models. \\
In general, this is preventing each individual unit from memorising some answer.

% subsection CNNs (end)

\subsubsection{ZFNet}\label{sub:zfnet} % (fold)
We will not talk about this network itself, since it is not especially interesting, but rather interesting things
learned from it. It was established that different kernels turned on and off by differing amounts for different textures
/ colours. 
% subsection ZFNet (end)

\subsubsection{VGG}\label{sec:vgg} % (fold)
This was an interesting paper on Very Deep Convolutional Networks for Large Scale Image Recognition. Here they
theorised instead of using large kernels between layers, we can use lots of smaller kernels, which will together
have the same receptive field as a larger kernel. For example, 3 $3 \times 3$ kernels have the same receptive field as a
$7 \times 7$ kernel. This is good since we have more non linearities (deeper), and fewer parameters: \[
    3 \cdot \left(3 ^ {2} C ^ {2}\right) = 27 C ^ {2} < 7^2 C^2 = 49 C^2
\]
Where $C$ is the number of input and output channels. This also means that there is no local response normalisation, adn
is a good basis for transfer learning. 
% subsubsection VGG (end)

\subsubsection{GoogLeNet}\label{sec:googlenet} % (fold)
Then Google (surprise surprise) created GoogLeNet, where they basically went \enquote{let's go deeper}. It includes
deeper networks, with 22 layers, greater computational efficiency. They are
not fully connected, and have fewer parameters, with 5M parameters, in comparison to AlexNet which has 60M, and VGG16
which has 138M. It also uses something called the \textit{inception module}.

The \textit{inception model} may be naïvely seen as: 
\begin{figure}[H]
    \center
    \includegraphics[scale=0.2]{tutorial_9_inception_model.png}
    \caption{Inception model, naïve version}
\end{figure}

A more complex model adds a dimension reductoin, but adding more convolutions to each of the 3 rightmost convolutions.
% subsubsection GoogLeNet (end)

\subsubsection{ResNet}\label{sec:resnet} % (fold)
Let $F \left(x\right)$ be a sequence of regular layers. In a \textit{residual block}, we merge $F \left(x\right)$ with
the input $x$, and return \[
    H \left(x\right) = F \left(x\right) + x
\]
The addition of $x$ to the output of the computation is called \textit{skip connection}. A residual block can contain
any number of layers and any type.

It was noticed that for a 56 layer model, there was a greater loss than for a 20 layer model, on CIFAR-10 ResNet was
created to resolve this, it is composed of  asequence of residual blocks, in addition to standard layers. It allows
training very deep networks with \begin{itemize}
    \item Vanishing gradient
    \item Shallow to deep architecture
\end{itemize}
For example, with 152 layers, it uses batch normalization after each convolution layer.
% subsubsection ResNet (end)

\subsection{Other tasks}\label{sub:other_tasks} % (fold)
There are other computer vision tasks, such as classifying and detecting objects in an image (like fruit, dogs, people,
ties, number plates, and so on). We can also use these models to do semantic segmentation, where we identify different
parts of an image (i.e. the person riding a horse, the horse, and other horses in the field).
% subsection Other tasks (end)
% section Classification (end)

\end{document}
