\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{tabularx}
\usepackage[autostyle=true]{csquotes}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\B}{\ensuremath{\mathbbm{B}}}

\title{Lecture 6 - Alignment}
\author{Gidon Rosalki}
\date{2025-11-23}


\begin{document}
\maketitle
\section{Alignment}\label{sec:alignment} % (fold)
We have been discussing finding the transformation between two images, i.e. the difference in translation, rotation,
zoom. There are also affine transformations, and homography transformations. Our assumption is that it is a static
scene, with no 3D effects like motion parallax. This is good for video stabilisation, denoising, video mosaicing, and so
on.

We discussed the different types of transformations at length in last week's tutorial. In short, we had translation,
scaling / zoom, rotation, affine, and projective / homography. Most affine transformations cannot be generated by
changing the camera parameters.

\subsection{Rolling shutter}\label{sub:rolling_shutter} % (fold)
In a film camera, an entire frame is recorded at the same time. This was similarly true with the first digital cameras
(used CCD). In newer digital cameras, like all mobile phones, instead of CCD, they use CMOS, where each line is recorded
at a different time. In a 24fps video, there may be 1000 lines in the camera sensor, so it records 24,000 lines per
second for a $1K \times 1K$ image. There's a very good video by Smarter Every Day explaining rolling shutter if it is
unclear. Overall, closer objects to the image move relatively faster, resulting in a greater slant. 

To reduce the effect of rolling shutter, one may use a very short exposure time, since then there will be less change to
the image between the top line, and the bottom line. However, in this course, we are going to pretend that there is no
rolling shutter.
% subsection Rolling shutter (end)

\subsection{Computing global translation}\label{sub:computing_global_translation} % (fold)
\subsubsection{Point correspondences}\label{sec:point_correspondences} % (fold)
Assume that we can find two corresponding point pairs, between two images. We can then compute the translation between
these two points, and thus find a way to match them together. The problem with this is that there may be many wrong
correspondences, or even \textit{no} correspondences. Consider an image with many repeating sections, like an image of a
circle. There are no points, and many repeating sections. 
% subsubsection Point correspondences (end)

\subsubsection{Direct methods}\label{sec:direct_methods} % (fold)
We are assuming constant brightness, and no rolling shutter. \\ 

Given images $I_1, I_2$, we can find the translation $\left(u, v\right)$, that will minimise the SSD (Sum of Squared
Differences): \[
    E \left(u, v\right) = \displaystyle\sum_{x}^{} \displaystyle\sum_{y}^{} \left(I_1 \left(x, y\right) - I_2 \left(x +
    u, y + v\right)\right)^2
\]
the implementation is to use the average \textbf{per pixel} error, only over an \textbf{area of overlap} (meaning, not
measuring the difference over where the images do not overlap). However, we also must divide by the number of pixels in
the area of overlap, so that the score does not change unduly by decreasing the area of overlap.

Since \[
    \left(a - b\right)^ {2} = a^2 - 2ab + b^2
\]
we can write \begin{align*}
    E \left(u, v\right) &= \displaystyle\sum_{x}^{} \displaystyle\sum_{y}^{}I_1^2 - 2 \displaystyle\sum_{x}^{}
    \displaystyle\sum_{y}^{} I_1 \left(x, y\right) \cdot I_2 \left(x + u, y + v\right) + \displaystyle\sum_{x}^{}
    \displaystyle\sum_{y}^{} I_2^2
\end{align*}
Since the first, and last terms are both almost constant, minimising the SSD is simply maximising the cross correlation
\[
    CC \left(u, v\right) = \displaystyle\sum_{x}^{} \displaystyle\sum_{y}^{} I_1 \left(x, y\right) \cdot I_2 \left(x+u,
    y + v\right)
\]

However, consider comparing between an image, and a lighter version of it. Given this method, there will be a higher
correlation between it and a lightened version of itself image, than between a duplicate, since multiplying by a higher
value pixel results in a higher value. To avoid this, we create NCC, which is invariant to global addition, and
multiplication of intensity $I_2 = a \cdot I_1 + b$. \[
    NCC \left(u, v\right) = \displaystyle\frac{\displaystyle\sum \left(I_1 \left(x, y\right) - \hat{I}_1\right) \cdot
    \left(I_2 \left(x + u, y + c\right) - \hat{I}_2\right)}{\sqrt{\displaystyle\sum \left(I_1 \left(x, y\right) -
\hat{I}_1\right)^2} \sqrt{\displaystyle\sum \left(I_2 \left(x, y\right) -
\hat{I}_2\right)^2}}
\]
Where in short, we subtract the average grey level, and divide by the variance. 

There is also multiresolution search, where we make use of pyramids. We start with a very small version of both images,
and find the highest matching area. We can then go down a level to a higher resolution version, and search within the
same area, and repeat this until we reach the full size images. This way, we do not have so many computations of
comparisons, since we only search a small window every time.

Normalised Cross Correlation is an excellent method to find objects in pictures, and to track objects in video.
Multiresolution search (pyramids) is used in object search, and is not needed when tracking from one frame to another.

There are some limitations of correlation search. There is only discrete accuracy, checking every possible translation
in integer pixel values, there is no sub pixel accuracy. Furthermore, the Complexity increases exponentially with
numbers of parameters: A translation $\left(u, v\right)$ has complexity of $n^2$, rotation $\left(u, v, \alpha\right)$
of $n^3$, zoom $\left(u, v, \alpha, s\right)$ has $n^4$, and affine is $n^6$. 
% subsubsection Direct methods (end)

\subsubsection{Continuous approximation}\label{sec:continuous_approximation} % (fold)
This is the Lucas-Kanade (LK) approximation. A local Taylor approximation in 1D is given as \[
    f \left(x + u\right) \approx f \left(x\right) + f' \left(x\right) \cdot u + \dots
\]

This may be expanded into 2D for images: \[
    f \left(x + u, y + v\right) \approx f \left(x, y\right) + \displaystyle\frac{\partial f}{\partial x}\cdot u +
    \displaystyle\frac{\partial f}{\partial y} \cdot v + \dots  
\]
% subsubsection Continuous approximation (end)

\subsubsection{Alignment by error minimisation}\label{sec:alignment_by_error_minimisation} % (fold)
When $I_2$ is shifted relative to $I_1$, we want to find the translation $\left(u, v\right)$ by minimising SSD: \[
    E \left(u, v\right) = \displaystyle\sum_{x}^{} \displaystyle\sum_{y}^{} \left(I_1 \left(x, y\right) - I_2 \left(x +
    u, y + v\right)\right)^2
\]
To simplifu, we may look at a single pixel, and use the Taylor approximation \begin{align*}
    E \left(u, v\right) &= \left(I_1 \left(x, y\right) - I_2 \left(x + u, y + v\right)\right)^2 \\ 
                        &\approx \left(I_2 \left(x, y\right) + \displaystyle\frac{\partial I_2}{\partial x} \cdot x +
                        \displaystyle\frac{\partial I_2}{\partial y} \cdot v - I_1 \left(x, y\right)  \right) \\ 
                        &= \left(I_x \cdot y + I_y \cdot v + I_t\right) ^ {2} \\ 
\end{align*}
Where \begin{gather}
    I_x = \displaystyle\frac{\partial I_2}{\partial x} \\ 
    I_y = \displaystyle\frac{\partial I_2}{\partial y} \\ 
    I_t = I_2 - I_1
\end{gather}

So, writing it in the simple form, we get the error function \begin{align*}
    E \left(u, v\right) &= \left(I_x \cdot u + I_y \cdot v + I_t\right)^2
\end{align*}
Where $I_x$ is the $x$ derivative of $I_2$, $I_y$ is the $y$ derivative of $I_2$, and $I_t$ is the image difference $I_2
- I_1$. We are looking to find $\left(u, v\right)$ which minimise the error function. The same $\left(u, v\right)$ will
approximately minimise the Taylor approximation, but it is only accurate for very small values of $\left(u, v\right)$,
on the order of a single pixel, since it is only the first order Taylor approximation. For larger values of $\left(u,
v\right)$, we have the iterative approach.

% subsubsection Alignment by error minimisation (end)

\subsubsection{Iterative approach}\label{sec:iterative_approach} % (fold)
We compute the image derviatives $I_x, I_y$, and set $u, v$ both to 0. Compute once \[
    A = \begin{bmatrix}
        \displaystyle\sum_{}^{}I_x \cdot I_x & \displaystyle\sum_{}^{}I_x \cdot I_y \\
        \displaystyle\sum_{}^{}I_y \cdot I_x & \displaystyle\sum_{}^{}I_y \cdot I_y \\
    \end{bmatrix}
\]
We iterate until convergence ($I_t \approx 0$), by \begin{enumerate}
    \item Compute $b = \begin{bmatrix}
        \displaystyle\sum_{}^{}I_x \cdot I_t  \\
        \displaystyle\sum_{}^{}I_y \cdot I_t 
        \end{bmatrix}$
    \item Solve the equations to compute the residual motion $A \cdot \begin{bmatrix}
        du \\ 
        dv
        \end{bmatrix} = -b$
    \item Update motion $u, v$ with residual motion $u += du,\ v += dv$
    \item Warp $I_2$ towards $I_1$ with total motion $\left(u, v\right)$
\end{enumerate}

The power of iterations allows us to compute the derivatives only once on $I_1$. There are two stages in each iteration,
motion estimation (solving equations), and warping $I_2$ (usually backwards). This works even with poor motion
estimation, as long as it reduces the overall error. Warping of one image towards the other is done from the original
image using total motion, and not from previous image using residual motion. Repetitive warping blurs.
% subsubsection Iterative approach (end)

\subsubsection{Multiresolution}\label{sec:multiresolution} % (fold)
Lucas-Kanade assumes that corresponding pixels in the two images have same derivative. It works OK even if derivatives
are similar. However, this fails for very large motions. So, reducing the resolution blurs the images, creating similar
derivatives, even for larger motions.

If we compare feature points to LK, we find that in both cases we go over the image to compute partial derivatives. As a
result, there is a very similar complexity. When uniquely identifiable features points can be found, then they are
better, as they can be used to compute homographies. However, in blurry images, feature points may be difficult to find,
and so LK may be preferable. Overall, LK is more accurate in translation.
% subsubsection Multiresolution (end)

% subsection Computing global translation (end)

\subsection{Optical flow}\label{sub:optical_flow} % (fold)
Optical flow is when there is an individual motion for each pixel (like when there is parallax, or independently moving
objects). 

We have some key assumptions (shocking nobody): \begin{itemize}
    \item Colour consistency, there is no change in colour (this is changed to brightness in greyscale images)
    \item Small motion: The points do not move very far
\end{itemize}

Examining small windows around a pixel may not provide accurate motion. Consider a line that passes beyond the window.
We would be unable to differentiate between the line translating to the left, and translating up and to the left. This
is called the \textbf{aperture problem}.

\subsubsection{Correlation based optical flow}\label{sec:correlation_based_optical_flow} % (fold)
For each small region in one image, we search for the best correlation in the second image. This assumes that we do not
move too much, and thus do not need to search the entire image. If there is a large region, then there is a higher
probability of recognising accurate motion, but there will also be poor localisation (poor knowledge of what moved). A
smaller region results in better localisation, but poorer knowledge of motion. We use pyramids to reduce the search
area.
% subsubsection Correlation based optical flow (end)

\subsubsection{Correlation based pyramids for optical flow}\label{sec:correlation_based_pyramids_for_optical_flow} % (fold)
To perform this we create two Gaussian pyramids from the two input images, compute the optical flow using $5 \times 5$
regions on the smallest pyramid level, smooth the optical flow and use it as an initial guess for higher resolutions,
and continue to the next level, searching close to the guess from the higher resolution.
% subsubsection Correlation based pyramids for optical flow (end)

\subsubsection{Gradient based optical flow}\label{sec:gradient_based_optical_flow} % (fold)
Here, we compute $\left(u, v\right)$ using Lucas-Kanade between two corresponding regions. \begin{itemize}
    \item Large region: Accurate motion. Poor localization.
    \item Small region: Good localization. Poor motion.
\end{itemize}
We use pyramids to reduce search area.
% subsubsection Gradient based optical flow (end)

\subsubsection{Pyramids and iterative refinement}\label{sec:pyramids_and_iterative_refinement} % (fold)
\begin{itemize}
    \item Create two Gaussian pyramids from the two input images
    \item Iterative Lukas-Kanade on smallest images \begin{enumerate}
        \item Estimate velocity at each pixel by solving Lucas- Kanade equations in its neighborhood
        \item Warp I2 towards I1 using the estimated flow field
        \item Repeat until convergence
    \end{enumerate}
    \item Continue to next pyramid level.
\end{itemize}
% subsubsection Pyramids and iterative refinement (end)
% subsection Optical flow (end)

% section Alignment (end)





\end{document}
