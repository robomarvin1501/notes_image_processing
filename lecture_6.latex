\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{tabularx}
\usepackage[autostyle=true]{csquotes}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\B}{\ensuremath{\mathbbm{B}}}

\title{Lecture 6 - Alignment}
\author{Gidon Rosalki}
\date{2025-11-23}


\begin{document}
\maketitle
\section{Alignment}\label{sec:alignment} % (fold)
We have been discussing finding the transformation between two images, i.e. the difference in translation, rotation,
zoom. There are also affine transformations, and homography transformations. Our assumption is that it is a static
scene, with no 3D effects like motion parallax. This is good for video stabilisation, denoising, video mosaicing, and so
on.

We discussed the different types of transformations at length in last week's tutorial. In short, we had translation,
scaling / zoom, rotation, affine, and projective / homography. Most affine transformations cannot be generated by
changing the camera parameters.

\subsection{Rolling shutter}\label{sub:rolling_shutter} % (fold)
In a film camera, an entire frame is recorded at the same time. This was similarly true with the first digital cameras
(used CCD). In newer digital cameras, like all mobile phones, instead of CCD, they use CMOS, where each line is recorded
at a different time. In a 24fps video, there may be 1000 lines in the camera sensor, so it records 24,000 lines per
second for a $1K \times 1K$ image. There's a very good video by Smarter Every Day explaining rolling shutter if it is
unclear. Overall, closer objects to the image move relatively faster, resulting in a greater slant. 

To reduce the effect of rolling shutter, one may use a very short exposure time, since then there will be less change to
the image between the top line, and the bottom line. However, in this course, we are going to pretend that there is no
rolling shutter.
% subsection Rolling shutter (end)

\subsection{Computing global translation}\label{sub:computing_global_translation} % (fold)
\subsubsection{Point correspondences}\label{sec:point_correspondences} % (fold)
Assume that we can find two corresponding point pairs, between two images. We can then compute the translation between
these two points, and thus find a way to match them together. The problem with this is that there may be many wrong
correspondences, or even \textit{no} correspondences. Consider an image with many repeating sections, like an image of a
circle. There are no points, and many repeating sections. 
% subsubsection Point correspondences (end)

\subsubsection{Direct methods}\label{sec:direct_methods} % (fold)
We are assuming constant brightness, and no rolling shutter. \\ 

Given images $I_1, I_2$, we can find teh translation $\left(u, v\right)$, that will minimise the SSD (Sum of Squared
Differences): \[
    E \left(u, v\right) = \displaystyle\sum_{x}^{} \displaystyle\sum_{y}^{} \left(I_1 \left(x, y\right) - I_2 \left(x +
    u, y + v\right)\right)^2
\]
the implementation is to use the average \textbf{per pixel} error, only over an \textbf{area of overlap} (meaning, not
measuring the difference over where the images do not overlap). However, we also must divide by the number of pixels in
the area of overlap, so that the score does not change unduly by decreasing the area of overlap.

Since \[
    \left(a - b\right)^ {2} = a^2 - 2ab + b^2
\]
we can write \begin{align*}
    E \left(u, v\right) &= \displaystyle\sum_{x}^{} \displaystyle\sum_{y}^{}I_1^2 - 2 \displaystyle\sum_{x}^{}
    \displaystyle\sum_{y}^{} I_1 \left(x, y\right) \cdot I_2 \left(x + u, y + v\right) + \displaystyle\sum_{x}^{}
    \displaystyle\sum_{y}^{} I_2^2
\end{align*}
Since the first, and last terms are both almost constant, minimising the SSD is simply maximising the cross correlation
\[
    CC \left(u, v\right) = \displaystyle\sum_{x}^{} \displaystyle\sum_{y}^{} I_1 \left(x, y\right) \cdot I_2 \left(x+u,
    y + v\right)
\]

However, consider comparing between an image, and a lighter version of it. Given this method, there will be a higher
correlation between it and a lightened version of itself image, than between a duplicate, since multiplying by a higher
value pixel results in a higher value. To avoid this, we create NCC, which is invariant to global addition, and
multiplication of intensity $I_2 = a \cdot I_1 + b$. \[
    NCC \left(u, v\right) = \displaystyle\frac{\displaystyle\sum \left(I_1 \left(x, y\right) - \hat{I}_1\right) \cdot
    \left(I_2 \left(x + u, y + c\right) - \hat{I}_2\right)}{\sqrt{\displaystyle\sum \left(I_1 \left(x, y\right) -
\hat{I}_1\right)^2} \sqrt{\displaystyle\sum \left(I_2 \left(x, y\right) -
\hat{I}_2\right)^2}}
\]
Where in short, we subtract the average grey level, and divide by the variance. 

There is also multiresolution search, where we make use of pyramids. We start with a very small version of both images,
and find the highest matching area. We can then go down a level to a higher resolution version, and search within the
same area, and repeat this until we reach the full size images. This way, we do not have so many computations of
comparisons, since we only search a small window every time.

Normalised Cross Correlation is an excellent method to find objects in pictures, and to track objects in video.
Multiresolution search (pyramids) is used in object search, and is not needed when tracking from one frame to another.

There are some limitations of correlation search. There is only discrete accuracy, checking every possible translation
in integer pixel values, there is no sub pixel accuracy. Furthermore, the Complexity increases exponentially with
numbers of parameters: A translation $\left(u, v\right)$ has complexity of $n^2$, rotation $\left(u, v, \alpha\right)$
of $n^3$, zoom $\left(u, v, \alpha, s\right)$ has $n^4$, and affine is $n^6$. 
% subsubsection Direct methods (end)

\subsubsection{Continuous approximation}\label{sec:continuous_approximation} % (fold)
This is the Lucas-Kanade (LK) approximation. A local Taylor approximation in 1D is given as \[
    f \left(x + u\right) \approx f \left(x\right) + f' \left(x\right) \cdot u + \dots
\]

This may be expanded into 2D for images: \[
    f \left(x + u, y + v\right) \approx f \left(x, y\right) + \displaystyle\frac{\partial f}{\partial x}\cdot u +
    \displaystyle\frac{\partial f}{\partial y} \cdot v + \dots  
\]
% subsubsection Continuous approximation (end)

\subsubsection{Alignment by error minimisation}\label{sec:alignment_by_error_minimisation} % (fold)
When $I_2$ is shifted relative to $I_1$, we want to find the translation $\left(u, v\right)$ by minimising SSD: \[
    E \left(u, v\right) = \displaystyle\sum_{x}^{} \displaystyle\sum_{y}^{} \left(I_1 \left(x, y\right) - I_2 \left(x +
    u, y + v\right)\right)^2
\]
To simplifu, we may look at a single pixel, and use the Taylor approximation \begin{align*}
    E \left(u, v\right) &= \left(I_1 \left(x, y\right) - I_2 \left(x + u, y + v\right)\right)^2 \\ 
                        &\approx \left(I_2 \left(x, y\right) + \displaystyle\frac{\partial I_2}{\partial x} \cdot x +
                        \displaystyle\frac{\partial I_2}{\partial y} \cdot v - I_1 \left(x, y\right)  \right) \\ 
                        &= \left(I_x \cdot y + I_y \cdot v + I_t\right) ^ {2} \\ 
\end{align*}
Where \begin{gather}
    I_x = \displaystyle\frac{\partial I_2}{\partial x} \\ 
    I_y = \displaystyle\frac{\partial I_2}{\partial y} \\ 
    I_t = I_2 - I_1
\end{gather}

So, writing it in the simple form, we get the error function \begin{align*}
    E \left(u, v\right) &= \left(I_x \cdot u + I_y \cdot v + I_t\right)^2
\end{align*}
Where $I_x$ is the $x$ derivative of $I_2$, $I_y$ is the $y$ derivative of $I_2$, and $I_t$ is the image difference $I_2
- I_1$. We are looking to find $\left(u, v\right)$ which minimise the error function. The same $\left(u, v\right)$ will
approximately minimise the Taylor approximation, but it is only accurate for very small values of $\left(u, v\right)$,
on the order of a single pixel, since it is only the first order Taylor approximation.

% subsubsection Alignment by error minimisation (end)

% subsection Computing global translation (end)

% section Alignment (end)





\end{document}
