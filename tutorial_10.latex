\documentclass[a4paper]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{tabularx}
\usepackage[autostyle=true]{csquotes}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\B}{\ensuremath{\mathbbm{B}}}

\title{Tutorial 10}
\author{Gidon Rosalki}
\date{2025-12-31}


\begin{document}
\maketitle
\section{Fine tuning}\label{sec:fine_tuning} % (fold)
Training a model from scratch has a few challenges. It takes an incredibly long time, since improvement is a slow
process, and there are many weights (millions) to train. It needs as a result huge amounts of electricity, and data,
which can be expensive, and hard to attain accordingly. The idea of fine tuning is to take a model that has already been
trained on a task, and adjust it to our intended task. This way, we need far less data, and time to train.

For example, we could start with a model trained on a large dataset (such as ImageNet or MNIST), keep the learned
features (edges, shapes, textures), and then train it with a smaller dataset. We need to be careful to only train it a
little, in order to avoid overfitting. 
% section Fine tuning (end)

\section{Low Rank Adaption - LoRA}\label{sec:lora} % (fold)
The key idea is to adapt big models with small changes. This is a method for fine tuning large models
\textit{efficiently}. Instead of updating all weights, LoRA adds small trainable matrices (which are low rank updates).
The original model weights stay frozen, it simply learns a few extra parameters. This is much faster, and needs less
memory than full fine tuning. The resultant model involves: \begin{gather}
    \text{Frozen: } W \in \R ^ {n \times m} \\
    \text{Trained: } A \in \R ^ {r \times m},\ B \in \R ^ {n \times r} \\ 
    r \text{ is usually very small, so 4 or 8: } r \ll n, m \\ 
    h = \left(W + BA\right)x
\end{gather}
% section LoRA (end)

\section{Generative models}\label{sec:generative_models} % (fold)
There are two main types of model. Discriminative (aimed at classification or regression) which attempts to learn $p
\left(y | x\right)$, and maps inputs to labels / values. There are also \textbf{generative} models, which learn $p
\left(x\right)$ or $p \left(x, y\right)$, where the idea is how to model, or \textit{generate} new data.

\subsection{Loss functions}\label{sub:loss_functions} % (fold)
For a model with the output of an image, rather than a single number or class label, we need to use different types of
loss functions. Firstly we have $L_1$: \begin{gather}
    L_1 \left(I_t, I_o\right) = \left|\left|I_t - I_o\right|\right|_1 = \displaystyle\frac{1}{N} \displaystyle\sum_{x,
    y}^{} \left|I_t \left[x, y\right] - I_o \left[x, y\right]\right|
\end{gather}
$I_t$ is the target image, and $I_o$ is the model output image. This gives sharper, but possibly noisier results. We
also have $L_2$: \begin{gather}
    L_1 \left(I_t, I_o\right) = \left|\left|I_t - I_o\right|\right|_2 ^ {2} = \displaystyle\frac{1}{N} \displaystyle\sum_{x,
    y}^{} \left(I_t \left[x, y\right] - I_o \left[x, y\right]\right) ^ {2}
\end{gather}
$L_2$ will give blurrier, but globally accurate results. 

There is also perceptual loss, which is feature based. It measures the difference between images in a deep feature space
(pre trained VGG / ResNet), encouraging the generated image to look perceptually similar to the target, rather than just
matching the pixel values: \[
    L_{VGG} \left(I_t, I_o\right) = \left|\left|\phi_l \left(I_t\right) - \phi_l \left(I_o\right)\right|\right|_2 ^ {2}
\]
We may see the differences in the below example:
\begin{figure}[H]
    \center
    \includegraphics[scale=0.2]{tutorial_10_loss_diff}
    \caption{Loss differences}
\end{figure}

% subsection Loss functions (end)

\subsection{Unsupervised learning}\label{sub:unsupervised_learning} % (fold)
Generally, we use something called unsupervised learning. We generate new samples from the same distribution of the
training data. A generative model needs to be Sufficient: for every image $x$ there must be $z : G \left(z\right) = x$,
and compact, for every $z$, $G \left(z\right)$ should be a valid image in $X$. There are many ways to address this
problem: \begin{itemize}
    \item Auto-regressive pixel generation
    \item VAE – variational autoencoders
    \item GAN – generative adversarial networks
    \item ViT – vision transformers
    \item Diffusion models
\end{itemize}

\subsubsection{Auto-Regressive pixel generation}\label{sub:auto_regressive_pixel_generation} % (fold)
There exists PixelCNN, which uses all previously generated pixels to predict the next one, using auto-regressive
convolutions. This creates nice results, but is very slow, since it only generates one pixel at a time: \[
    p \left(x\right) = \displaystyle\prod_{i = 1}^{n ^ {2}}p \left(x_i | x_1, \dots, x_{i - 1}\right)
\]
% subsection Auto-Regressive pixel generation (end)

\subsubsection{VAE - Variational autoencoders}\label{sec:vae_variational_autoencoders} % (fold)
An autoecnoder compresses the input to a low dimensional latent vector. This is done with unsupervised learning, adn the
loss is \[
    \left|\left|x - \widehat{x}\right|\right| ^ {2}
\]
The latent vector can be the input to a supervised classification network. 
\begin{figure}[H]
    \center
    \includegraphics[scale=0.2]{tutorial_10_autoencoders}
    \caption{Autoencoders}
\end{figure}

So, for VAE we sample latent vectors to create new images. We need to enforce some \enquote{arrangement} on the latent
space, so we encourage teh latent vector to be normally distributed. Instead of learning the latent vector, we learn the
mean standard deviation, and use them to sample from the normal distribution. The loss would now be both the
reconstruction loss $\left|\left|x - \widehat{x}\right|\right| ^ {2}$, and the regularisation loss $KL \left[N
\left(\mu_x, \sigma_x\right) \| N \left(0, 1\right)\right]$. 
% subsubsection VAE - Variational autoencoders (end)

\subsubsection{GAN - generative adversarial networks}\label{sec:gan_generative_adversarial_networks} % (fold)
This is a two player game: \begin{itemize}
    \item Generator network $G$: This generates images that will look real enough to fool $D$ 
    \item Discriminator network $D$: Distinguish between real images, and images produced by $G$
\end{itemize}
Both networks are trained together in a minmax game, the discriminator is trained to maximise the difference between
real and fake images, and the generator is trained to minimise it: \[
    \displaystyle\min_{G} \left\{\displaystyle\max_{D} \left\{V \left(G, D\right)\right\} \right\}  = \E_{x \sim
    p_{data} \left(x\right)} \left[\log D \left(x\right)\right] + \E_{z \sim p_z \left(z\right)} \left[\log \left(1 - D
\left(G \left(z\right)\right)\right)\right]
\]
So the discriminator is trained to maximise the difference between its output on real images (such that for image $x$,
$D \left(x\right) \to 1$), to the output on generated images (such that for image $G \left(z\right)\ D \left(G
\left(z\right)\right) \to 0$). In contrast, $G$ is trained to minimise it, such that $D \left(G \left(z\right)\right)
\to 1$. \\ 
They are trained using an iterative approach. You begin by fixing the generator, and training the discriminator: \[
    % TODO 23
\]

Training GANs is very difficult, and unstab;e. It does not always converge, since parameters are unstable, so they will
oscillate when training and may never converge. Additionally, they can undergo mode collapse, where the generator always
produces the same kind of images. Finally, a strong discriminator causes small vanishing gradients for the generator.
They are also very sensitive to choice of hyperparameters. 
% subsubsection GAN - generative adversarial networks (end)

\subsubsection{StyleGAN}\label{sec:stylegan} % (fold)
We want better understanding of GANs and latent space. StyleGAN incorporates ideas from style transfer to separate high
level attributes (pose, identity, etc.) from stochastic variation (freckles, hair, etc. ) in generated images. This only
modifies the generator. Instead of using $z$ as the input the to generator, we first map it
to an intermediate latent space $W$ using a non-linear mapping $f: Z \to W$. $w$ will be inserted into the generator as
a style \enquote{hint}, and noise inputs are also given to the generator to help it generate stochastic detail. 
% subsubsection StyleGAN (end)

\subsubsection{ViT - vision transformers}\label{sec:vit_vision_transformers} % (fold)
Let us begin by discussing CLIP. CLIP stands for Contrastive Language Image Pretraining. It's goal is to teach a model
to understand which text matches which image. Instead of predicting models, CLIP learns to match the correct image, with
the correct text, and separate mismatched pairs.

The \textbf{attention mechanism} was originally developed for NLP tasks. Since not all words in a sentence are equally
important, attention lets the model focus on relevant words. Attention works by havign each word create a Query, Key,
Value vector. It scores how much one word relates to another, and the weighted sum gives a context vector for each word.
This enables the model to capture meaning across the whole sentence. \begin{gather}
    \text{Attention} \left(Q, K, V\right) = \text{softmax} \left(\displaystyle\frac{Q K ^ {T}}{\sqrt{d_k} }\right) V \\
\end{gather}

There is also \textit{cross attention}, which is a mechanism where one set of features attends to another set. This lets
information flow between two different sources. 

So, ViT has us take an image, and separate it into patches. We then multiply each vectorised patch by a matrix to
transfer us to some other space, these are passed through embeddings, and we have build a transformer encoder that
classifies the patches.  
% subsubsection ViT - vision transformers (end)

\subsubsection{Diffusion models}\label{sec:diffusion_models} % (fold)
Diffusion models learn to invert a parameterised Markovian image noising process. This can generate high quality images.
More simply, they take random noise, and slowly de-noise them to generate high quality images. 
\begin{figure}[H]
    \center
    \includegraphics[scale=0.2]{tutorial_10_denoising}
    \caption{Denoising}
\end{figure}
In general, in generative models, we try to learn the probability distribution $p \left(x\right)$. So in diffusion
models, what we learn is the \enquote{direction} each point should move in order to reach the true distribution.

There are a couple of problems: \begin{enumerate}
    \item The model is unconditioned, so it generates a random image without any prompt or guidance.
    \item For large images, the generation process takes a long time.
\end{enumerate}

% subsubsection Diffusion models (end)

% subsection Unsupervised learning (end)

% section Generative models (end)

\end{document}
