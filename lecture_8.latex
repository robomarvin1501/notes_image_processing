\documentclass[a4paper]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{blindtext}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{float}
\usepackage[bookmarks=true]{hyperref}
\usepackage{bookmark}
\usepackage{bbm}
\usepackage{wrapfig}
\usepackage{tabularx}
\usepackage[autostyle=true]{csquotes}
\graphicspath{ {./images/} }
\usepackage[bottom=0.5cm, right=1.5cm, left=1.5cm, top=1.5cm]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\B}{\ensuremath{\mathbbm{B}}}

\title{Lecture 8 - Compression}
\author{Gidon Rosalki}
\date{2025-12-07}


\begin{document}
\maketitle

\section{Introduction}\label{sec:introduction} % (fold)
The purpose of compression is to reduce the number of bits needed to represent an image. Consider, a still image of $1K
\times 1K$, each pixel comprised of 24 bits, and we will need 3MB for each image. Therefore, 1 second of HD video, at 25
frames per second will need 75MB per second, and so an hour would need 270GB. We know that it is possible to compress
this, because today an hour of HD video only needs around 1GB of storage space. \\
There is possibility of spatial correlation, pixels that are close in space are similar, so less information can be
stored. Additionally there is temporal correlation in videos, frames that are close to each other in time are often
similar.

There are two main types of compression, lossless and lossy. In lossless, there is no information loss, and the image
may be reconstructed exactly as the original. This is often applied in medical imaging, archiving, and may be applied to
any file, such as text. An image example is png files. \\
However, lossy compression allows small changes from the original. This may be applied to all photography, and is
designed specifically for images and video.
% section Introduction (end)

\section{Huffman encoding}\label{sec:huffman_encoding} % (fold)
Huffman is a form of lossless compression. In the first pass, it measures the distribution of the values, and in the
second pass uses a variable length code where rare values get longer codes, and popular values get shorter codes. This
reduces the total number of bits required to represent an image, and is effective when different values have different
probabilities. It can be used to encode all of English with approximately 1 bit per letter.

So, how do we perform this? It involves a path in a tree. Let us assume that our pixels have colours $a_1, a_2, \dots,
a_n$ with probabilities $p_1, \dots, p_n$. We then build a tree by iteratively joining two nodes with the lowest
probabilities. The colours of the pixels are now the leaves in the tree, and the Huffman code is an encoding of the path
from the root of hte tree to each leaf. Colours that appear frequently get a short code, and rarer values get longer
codes.

Consider the following table: \begin{table}[H]
     \centering
     \begin{tabular}{|c|c|c|c|c|}
         \hline
         Pixel value & 1 & 2 & 3 & 4 \\ \hline
         Probability & 0.1 & 0.05 & 0.05 & 0.8 \\ \hline
         Huffman code & 11 & 101 & 100 & 0 \\ \hline
         Code length & 2 & 3 & 3 & 1 \\ \hline
     \end{tabular}
     \caption{Huffman table}
\end{table} 
So here, if we encode 1,000 pixels with this distribution, a fixed length code would need 2 bits to encode 4 values,
resulting in a total of 2,000 bits. However, with Huffman, we would only need \[
    800 \cdot 1 + 100 \cdot 2 + 100 \cdot 3 = 1300
\]
A significant reduction. If all the colours have equal probabilities, then we will produce a uniform length code, and
there will be no compression. A question from an exam, how can one compress a binary image using Huffman? One cannot,
since one still needs at least two bits. By definition, Huffman codes cannot help binary images.
% section Huffman encoding (end)

\section{Entropy}\label{sec:entropy} % (fold)
\subsection{Shannon encoding}\label{sub:shannon_encoding} % (fold)
Given a probability distribution $p \left(\cdot\right)$ over events $0 \leq k \leq n$, how much new information is
received when we are told that event $k$ occurred? Common events carry very little information, but rare events, much
more. Shannon encoded information as $- \log_2 p \left(k\right)$. So a high value of $p \left(k\right)$ is very little
information, but a low $p \left(k\right)$ implies a lot of information. Since each $k$ comes with the probability $p
\left(k\right)$, the average new information for the distribution is given as \[
    \text{Entropy } = H \left(X\right) = \displaystyle\sum_{k = 0}^{n - 1} p \left(k\right) \log p \left(k\right)
\]
Entropy gives the information per pixel for a distribution. The distribution with the highest certainty would be $p
\left(k\right) = 0$ except for $p \left(0\right)= 1$. This has the entropy of 0, so absolute certainty. The distribution
with the maximum uncertainty is the uniform distribution, with $p \left(k\right) = \displaystyle\frac{1}{n}$, and the
entropy of $\log \left(n\right)$. \\ 
To draw an analogy to physics, systems with lower temperatures have lower entropy, and systems with higher temperatures
have higher entropy. 
% subsection Shannon encoding (end)

\subsection{Predictive encoding}\label{sub:predictive_encoding} % (fold)
This predicts a pixel value, and then transmits the \textit{difference} from the prediction. For example, a linear
predictor from earlier raster pixels: \begin{gather}
    \begin{bmatrix}
        y_2 & y_3 \\
        y_1 & x
    \end{bmatrix} \\
    x = \displaystyle\frac{y_1 + y_3}{2}
\end{gather}
The original picture may then be recovered from the predicted differences. The histogram of differences has much lower
entropy than the original histogram, and so we can get a much better Huffman encoding of it than of the original image.
% subsection Predictive encoding (end)
% section Entropy (end)

\section{Lossless compression}\label{sec:lossless_compression} % (fold)
To perform lossless compression we: \begin{enumerate}
    \item create a predictive encoding to reduce entropy
    \item Select and perform a favoured lossless encoding: \begin{enumerate}
            \item Huffman coding 
            \item Lempel Ziv
        \end{enumerate}
    \item Pack the results in a dedicated format with the checksum, and so on
\end{enumerate}

Let us consider the following two images: 
\begin{figure}[H]
    \center
    \includegraphics[scale=0.2]{lecture_8_matches}
    \caption{}
\end{figure}
Despite the fact that the left image has a high entropy, and the right a low, they have a very similar histogram, and so
Huffman will give very similar encodings for the two of them. We can do better by using Lempel-Ziv compression (LZW).

\subsection{LZW}\label{sec:lzw} % (fold)
This is particularly powerful with repetitive patterns. It is a one pass, online algorithm, that builds an on the fly
dictionary of strings. The index of entries in the dictionary are transmitted (here the pixel value of 0 to 255 is
treated s a character). The same dictionary is built at the receiving side. However, be warned, there are several
different variants. Let us consider the following example algorithm: % \usepackage{algorithm,algorithmicx,algpseudocode}
\begin{algorithm}[H]
    \floatname{algorithm}{LZW}
    \algrenewcommand\algorithmicrequire{\textbf{Input: }}
    \algrenewcommand\algorithmicensure{\textbf{Output: }}
    \caption{}\label{alg:}
    \begin{algorithmic}[1]
        \Require $input$
        \Ensure $output$
        \State Initialise a dictionary with all single characters (0..255)
        \State P = first input character 
        \While{Not at end of input stream}
            \State C = next input character
            \If{PC is in dictionary}
                \State P = PC
            \Else
                \State output the code for P 
                \State Add PC to the dictionary 
                \State P = C
            \EndIf
        \EndWhile
        \State \textbf{return} Output index for P
    \end{algorithmic}
\end{algorithm}
The important thing to note here is that LZW is very good at repetitive patterns, since they will be encoded once, and
referenced many times. 
% subsection LZW (end)

\subsection{Run Length Encoding - RLE}\label{sub:run_length_encoding_rle} % (fold)
This is mainly used in binary images. It encodes pairs, indicating the colour, and the length of that colour, for
example: \[
    \left(1, 63\right), \left(0, 87\right), \left(1, 37\right), \dots
\]
This works very well on large uniform regions like text. We can even remove the colour bit, since we know that it
changes every time there is a new input. We will note that this is not always an effective method for compression,
sometimes it may in fact increase the amount of data, depending on the input. We will also note that repeated
compression \textbf{does not help}, and will often \textbf{increase} the amount of data required.
% subsection Run Length Encoding - RLE (end)
% section Lossless compression (end)

\section{Lossy compression}\label{sec:lossy_compression} % (fold)
Here, we take the image, transform it, quantise it, encode, and then store / transmit it. To decompress we simply
decode, and run the inverse transform. The quantisation is the lossy part, and is the main source for compression but
also for errors. Errors are not easy to measure. We can use things like MLE, but it is imperfect. Consider the following
simple example: \begin{enumerate}
    \item Image Transform: Convolve with (1, -1)
    \item Quantization: None
    \item Coding: Huffman Coding
    \item Lossless coding
    \item Improved compression: after convolution with (1, -1) the colour distribution will be concentrated around zero.
\end{enumerate}

\subsection{JPEG}\label{sub:jpeg} % (fold)
For JPEG, we split an image into non overlapping $n \times n$ blocks, and perform a transform on those blocks to give $n
\times n$ transform values. We then quantise, and encode these values. JPEG uses $8 \times 8$ blocks, with the Discrete
Cosine Transform (DCT), uniform quantisation, and Huffman encoding.

For the 64 ($8 \times 8$) DCT basis function, each 8 by 8 image block is represented as a weighted sum of the basis
functions. The 2D DCT transform finds those weights, and low weights (of high frequencies) are zeroed. 
\begin{figure}[H]
    \center
    \includegraphics[scale=0.2]{lecture_8_dct_basis}
    \caption{DCT Basis}
\end{figure}
We use the DCT, in place of the DFT since DCT is better for compression than the DFT. Reflection in place of duplication
at the edges reduces the high frequencies, and the DCT has real coefficients, in place of the DFT's complex
coefficients.

\subsubsection{Colours}\label{sec:colours} % (fold)
For colours in JPEG we transform from RGB to $Y'C_BC_R$. Here, Y' is the intensity, a weighted sum of R, G, and B. $C_B,
C_R$ are chromaticity values by colour differences, so \begin{gather}
    C_B = B - \left(R + G\right) \\
    C_R = R - \left(B + G\right) \\
\end{gather}
% subsubsection Colours (end)

% subsection JPEG (end)
% section Lossy compression (end)



\end{document}
